"""
è¯­éŸ³è¯†åˆ«ç®¡é“

æä¾›ä»éŸ³é¢‘è¾“å…¥åˆ°æ–‡æœ¬è¾“å‡ºçš„å®Œæ•´å¤„ç†æµç¨‹
"""

import os
import json
import queue
import logging
import numpy as np
from typing import Tuple, Optional
from vosk import Model, KaldiRecognizer
import sounddevice as sd
from ..models.voice_models import AudioData, SpeechRecognitionResult

logger = logging.getLogger(__name__)

# è‡ªåŠ¨å®šä½æ¨¡å‹è·¯å¾„
ROOT_DIR = os.path.dirname(
    os.path.dirname(
        os.path.dirname(os.path.abspath(__file__))
    )
)
MODEL_PATH = os.path.join(ROOT_DIR, "vosk-model-cn-0.22")

if not os.path.exists(MODEL_PATH):
    raise RuntimeError(f"Vosk æ¨¡å‹æœªæ‰¾åˆ°: {os.path.abspath(MODEL_PATH)}")


class SpeechRecognitionPipeline:
    """è¯­éŸ³è¯†åˆ«ç®¡é“"""

    def __init__(self, sample_rate: int = 16000):
        """
        åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«ç®¡é“

        å‚æ•°:
            sample_rate: éŸ³é¢‘é‡‡æ ·ç‡
        """
        self.sample_rate = sample_rate
        self.model = Model(MODEL_PATH)
        logger.info("Vosk æ¨¡å‹åŠ è½½å®Œæˆ")

    def recognize_from_file(self, audio_file: str) -> SpeechRecognitionResult:
        """
        ä»éŸ³é¢‘æ–‡ä»¶è¯†åˆ«è¯­éŸ³

        å‚æ•°:
            audio_file: éŸ³é¢‘æ–‡ä»¶è·¯å¾„

        è¿”å›:
            è¯­éŸ³è¯†åˆ«ç»“æœ
        """
        # TODO: å®ç°ä»æ–‡ä»¶è¯†åˆ«çš„åŠŸèƒ½
        raise NotImplementedError("ä»æ–‡ä»¶è¯†åˆ«åŠŸèƒ½å°šæœªå®ç°")

    def recognize_from_audio(
        self, 
        audio_data: np.ndarray,
        sample_rate: Optional[int] = None
    ) -> SpeechRecognitionResult:
        """
        ä»éŸ³é¢‘æ•°æ®è¯†åˆ«è¯­éŸ³

        å‚æ•°:
            audio_data: éŸ³é¢‘æ•°æ®
            sample_rate: éŸ³é¢‘é‡‡æ ·ç‡ï¼ˆå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨åˆå§‹åŒ–æ—¶çš„é‡‡æ ·ç‡ï¼‰

        è¿”å›:
            è¯­éŸ³è¯†åˆ«ç»“æœ
        """
        sr = sample_rate or self.sample_rate

        # åˆ›å»ºéŸ³é¢‘æ•°æ®å¯¹è±¡
        audio_obj = AudioData(audio_data, sr)

        # å¦‚æœé‡‡æ ·ç‡ä¸åŒ¹é…ï¼Œéœ€è¦é‡é‡‡æ ·
        if sr != self.sample_rate:
            # TODO: å®ç°é‡é‡‡æ ·
            logger.warning(f"é‡‡æ ·ç‡ä¸åŒ¹é…: {sr} vs {self.sample_rate}ï¼Œéœ€è¦é‡é‡‡æ ·")

        # åˆ›å»ºè¯†åˆ«å™¨
        recognizer = KaldiRecognizer(self.model, self.sample_rate)

        # å¤„ç†éŸ³é¢‘
        recognized_text = ""
        if isinstance(audio_data[0], np.int16):
            audio_bytes = audio_data.tobytes()
        else:
            audio_bytes = (audio_data * 32768).astype(np.int16).tobytes()

        if recognizer.AcceptWaveform(audio_bytes):
            result = json.loads(recognizer.Result())
            recognized_text = result.get("text", "").strip()
        else:
            result = json.loads(recognizer.PartialResult())
            partial = result.get("partial", "")
            if partial:
                recognized_text = partial

        # åˆ›å»ºè¯†åˆ«ç»“æœ
        recognition_result = SpeechRecognitionResult(
            text=recognized_text,
            confidence=1.0,  # Voskä¸æä¾›ç½®ä¿¡åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼
            is_final=True,
            audio_data=audio_obj
        )

        return recognition_result

    def listen_for_speech(
        self, 
        timeout: int = 30,
        pause_threshold: float = 1.2
    ) -> Tuple[SpeechRecognitionResult, np.ndarray]:
        """
        å®æ—¶ç›‘å¬è¯­éŸ³

        å‚æ•°:
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            pause_threshold: åœé¡¿é˜ˆå€¼ï¼ˆç§’ï¼‰

        è¿”å›:
            (è¯­éŸ³è¯†åˆ«ç»“æœ, éŸ³é¢‘æ•°æ®)
        """
        q = queue.Queue()
        audio_chunks = []
        recognized_text = ""

        def callback(indata, frames, time, status):
            if status:
                logger.warning(f"å½•éŸ³çŠ¶æ€: {status}")
            if not isinstance(indata, bytes):
                indata = bytes(indata)
            q.put(indata)
            audio_chunks.append(indata)

        print("ğŸ¤ è¯·å›ç­”ï¼ˆè¯´å®Œåç¨ä½œåœé¡¿å³å¯ï¼‰...")

        try:
            with sd.RawInputStream(
                samplerate=self.sample_rate,
                blocksize=8000,
                dtype='int16',
                channels=1,
                callback=callback
            ):
                recognizer = KaldiRecognizer(self.model, self.sample_rate)
                silence_count = 0
                total_time = 0.0

                while True:
                    try:
                        data = q.get(timeout=1.0)
                    except queue.Empty:
                        silence_count += 10
                        total_time += 1.0
                        if total_time > timeout:
                            break
                        continue

                    if not isinstance(data, bytes):
                        data = bytes(data)

                    total_time += 0.1

                    # ç´¯ç§¯è¯†åˆ«ç»“æœ
                    if recognizer.AcceptWaveform(data):
                        res = json.loads(recognizer.Result())
                        text_chunk = res.get("text", "").strip()
                        if text_chunk:
                            recognized_text += text_chunk + " "
                            silence_count = 0
                            print(f"ğŸ“ ä½ è¯´çš„æ˜¯: '{text_chunk}'")
                    else:
                        partial = json.loads(recognizer.PartialResult()).get("partial", "")
                        if not partial:
                            silence_count += 1
                        else:
                            silence_count = 0

                    if silence_count > 20 or total_time > timeout:
                        break

                # åˆå¹¶éŸ³é¢‘
                if audio_chunks:
                    full_bytes = b''.join(audio_chunks)
                    audio_int16 = np.frombuffer(full_bytes, dtype=np.int16)
                    audio_float32 = audio_int16.astype(np.float32) / 32768.0
                else:
                    audio_float32 = np.array([])

                if not recognized_text:
                    print("ğŸ“ æœªè¯†åˆ«åˆ°æœ‰æ•ˆè¯­éŸ³")
                else:
                    print(f"âœ… æœ€ç»ˆè¯†åˆ«ç»“æœ: '{recognized_text.strip()}'")

                # åˆ›å»ºè¯†åˆ«ç»“æœ
                recognition_result = SpeechRecognitionResult(
                    text=recognized_text.strip(),
                    confidence=1.0,
                    is_final=True
                )

                return recognition_result, audio_float32

        except Exception as e:
            logger.error(f"å½•éŸ³æˆ–è¯†åˆ«å¼‚å¸¸: {e}")
            print(f"âŒ è¯­éŸ³äº¤äº’å¤±è´¥: {e}")
            return SpeechRecognitionResult(text=""), np.array([])
