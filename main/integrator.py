"""
JingXin å¤šæ¨¡æ€é›†æˆå™¨

ç»Ÿä¸€è°ƒç”¨ face_expression, gesture_analysis, voice_interaction ä¸‰ä¸ªå­æ¨¡å—ï¼Œ
å®ç°é¢éƒ¨è¡¨æƒ…ã€æ‰‹åŠ¿å§¿æ€ã€è¯­éŸ³å†…å®¹çš„è”åˆåˆ†æã€‚
"""

import cv2
import time
import numpy as np
from typing import Dict, Any, Optional
import threading
import queue

# æ­£ç¡®çš„æ¨¡å—å¯¼å…¥è·¯å¾„ï¼ˆåŸºäºä½ çš„å®Œæ•´ç›®å½•ç»“æ„ï¼‰
try:
    # é¢éƒ¨åˆ†æ - ä½¿ç”¨ face_au_analyzer.py
    from face_expression.analyzers.face_au_analyzer import FaceAUAnalyzer

    # æ‰‹åŠ¿åˆ†æ - ä½¿ç”¨ hand_analyzer.py å’Œ shoulder_analyzer.py
    from gesture_analysis.analyzers.hand_analyzer import HandAnalyzer
    from gesture_analysis.analyzers.shoulder_analyzer import ShoulderAnalyzer
    from gesture_analysis.utils.visualization import Visualizer as GestureVisualizer

    # è¯­éŸ³äº¤äº’ - ä½¿ç”¨ speech_recognizer.py å’Œ tts_engine.py
    from voice_interaction.analyzers.speech_recognizer import SpeechRecognizer
    from voice_interaction.analyzers.tts_engine import TTSEngine
    from voice_interaction.assessment.interview_assessment import InterviewAssessment

except ImportError as e:
    print("âŒ æ¨¡å—å¯¼å…¥å¤±è´¥ï¼è¯·ç¡®ä¿ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼š")
    print("   python -m main.examples.run_integrated_interview")
    print(f"   é”™è¯¯è¯¦æƒ…: {e}")
    exit(1)


class JingXinIntegrator:
    """å¤šæ¨¡æ€é›†æˆå™¨"""

    def __init__(self):
        # è§†é¢‘åˆ†æç»„ä»¶
        self.cap = None
        self.mp_hands = None
        self.mp_pose = None
        self.face_analyzer = None
        self.hand_analyzer_left = None
        self.hand_analyzer_right = None
        self.shoulder_analyzer = None
        self.gesture_visualizer = None

        # è¯­éŸ³ç»„ä»¶
        self.speech_recognizer = None
        self.tts_engine = None
        self.interview_assessment = None

        # åˆ†æç»“æœç¼“å­˜
        self.current_results = {
            "face": {"is_valid": False},
            "hand": {"is_valid": False},
            "shoulder": {"is_valid": False, "is_calibrated": False},
            "emotion": {"overall_score": 50.0, "emotion_state": "ä¸­æ€§", "emoji": "ğŸŸ¡", "feedback": "ç³»ç»Ÿåˆå§‹åŒ–ä¸­", "color": (255, 255, 0)},
            "voice": {"text": "", "is_valid": False}
        }

        # çŠ¶æ€æ ‡å¿—
        self.running = False
        self.interview_started = False
        self.current_question_index = 0

        # åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶
        self._init_components()

    def _init_components(self):
        """åˆå§‹åŒ–æ‰€æœ‰åˆ†æå™¨å’Œå·¥å…·"""
        try:
            # åˆå§‹åŒ– MediaPipe æ¨¡å‹ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼‰
            import mediapipe as mp
            self.mp_hands = mp.solutions.hands.Hands(
                static_image_mode=False,
                max_num_hands=2,
                min_detection_confidence=0.7,
                min_tracking_confidence=0.5
            )
            self.mp_pose = mp.solutions.pose.Pose(
                static_image_mode=False,
                model_complexity=1,
                min_detection_confidence=0.6,
                min_tracking_confidence=0.6
            )

            # åˆå§‹åŒ–åˆ†æå™¨
            self.face_analyzer = FaceAUAnalyzer()
            self.hand_analyzer_left = HandAnalyzer(hand_id=0)
            self.hand_analyzer_right = HandAnalyzer(hand_id=1)
            self.shoulder_analyzer = ShoulderAnalyzer()

            # åˆå§‹åŒ–å¯è§†åŒ–å·¥å…·
            self.gesture_visualizer = GestureVisualizer()

            # åˆå§‹åŒ–è¯­éŸ³ç»„ä»¶
            self.speech_recognizer = SpeechRecognizer()
            self.tts_engine = TTSEngine()
            self.interview_assessment = InterviewAssessment()

            print("âœ… æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–æˆåŠŸ")

        except Exception as e:
            raise RuntimeError(f"ç»„ä»¶åˆå§‹åŒ–å¤±è´¥: {e}")

    def start_interview_session(self):
        """å¯åŠ¨é¢è¯•ä¼šè¯"""
        print("=" * 60)
        print("JingXin å¤šæ¨¡æ€é¢è¯•è¯„ä¼°ç³»ç»Ÿ")
        print("=" * 60)
        print("âœ… ç³»ç»Ÿå·²å¯åŠ¨ï¼Œæ­£åœ¨åˆå§‹åŒ–æ‘„åƒå¤´...")

        # æ‰“å¼€æ‘„åƒå¤´
        self.cap = cv2.VideoCapture(0)
        if not self.cap.isOpened():
            raise RuntimeError("æ— æ³•æ‰“å¼€æ‘„åƒå¤´ï¼")

        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

        # å¯åŠ¨å¼€åœºç™½
        self._start_introduction()

        # å¯åŠ¨ä¸»å¾ªç¯
        self.running = True
        self._main_loop()

    def _start_introduction(self):
        """æ’­æ”¾å¼€åœºç™½ï¼ˆåŒæ—¶é¢„é€‚åº”è§†é¢‘æµï¼‰"""
        print("" + "=" * 60)
        print("ğŸ¥ è§†é¢‘æµé¢„é€‚åº”é˜¶æ®µ")
        print("=" * 60)
        print("ç³»ç»Ÿæ­£åœ¨é¢„çƒ­æ‘„åƒå¤´å’Œåˆ†æå™¨...")
        print("è¯·ä¿æŒè‡ªç„¶åå§¿ï¼Œè®©ç³»ç»Ÿæ ¡å‡†è‚©éƒ¨åŸºå‡†çº¿")
        print("=" * 60 + "")

        # é¢„é€‚åº”é˜¶æ®µï¼šæŒç»­æ˜¾ç¤ºè§†é¢‘æµï¼Œè®©ç”¨æˆ·é€‚åº”
        warmup_frames = 0
        max_warmup_frames = 150  # çº¦5ç§’ï¼ˆ30fpsï¼‰

        while warmup_frames < max_warmup_frames:
            success, image = self.cap.read()
            if not success:
                continue

            # é•œåƒç¿»è½¬
            image = cv2.flip(image, 1)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # é¢„åˆ†æï¼ˆç”¨äºæ ¡å‡†ï¼‰
            self.analyze_frame(image_rgb, image)

            # æ˜¾ç¤ºæç¤ºä¿¡æ¯
            cv2.putText(image, "ç³»ç»Ÿé¢„çƒ­ä¸­...", (10, 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            cv2.putText(image, f"å‰©ä½™: {(max_warmup_frames - warmup_frames) // 30}ç§’", (10, 60),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
            cv2.imshow('JingXin Integrated Analyzer', image)

            warmup_frames += 1
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

        print("âœ… è§†é¢‘æµé¢„é€‚åº”å®Œæˆ")

        # æ’­æ”¾å¼€åœºç™½
        if self.tts_engine and self.tts_engine.is_available():
            self.tts_engine.speak("ä½ å¥½ï¼Œæ¬¢è¿å‚åŠ JingXinå¤šæ¨¡æ€é¢è¯•è¯„ä¼°ã€‚")
            self.tts_engine.speak("æˆ‘ä¼šåŒæ—¶åˆ†ææ‚¨çš„é¢éƒ¨è¡¨æƒ…ã€æ‰‹åŠ¿å§¿æ€å’Œè¯­éŸ³å†…å®¹ã€‚")
            self.tts_engine.speak("è¯·ä¿æŒè‡ªç„¶åå§¿ï¼Œç³»ç»Ÿå°†åœ¨5ç§’åå¼€å§‹æé—®ã€‚")
            time.sleep(5)
        self.interview_started = True

    def _main_loop(self):
        """ä¸»åˆ†æå¾ªç¯"""
        frame_count = 0
        last_report_time = time.time()

        while self.running and self.cap.isOpened():
            success, image = self.cap.read()
            if not success:
                continue

            # é•œåƒç¿»è½¬
            image = cv2.flip(image, 1)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # åˆ†æå½“å‰å¸§
            results = self.analyze_frame(image_rgb, image)

            # å¤„ç†è¯­éŸ³äº¤äº’ï¼ˆå¦‚æœé¢è¯•å·²å¼€å§‹ä¸”è¿˜æœ‰é—®é¢˜ï¼‰
            if self.interview_started and self.current_question_index < len(self.interview_assessment.questions):
                self._handle_voice_interaction()

            # å¯è§†åŒ–ç»“æœ
            image = self._visualize_all(image, results)

            # æ˜¾ç¤ºå›¾åƒ
            cv2.imshow('JingXin Integrated Analyzer', image)

            # ç»ˆç«¯æŠ¥å‘Šï¼ˆæ¯2ç§’ï¼‰
            current_time = time.time()
            if current_time - last_report_time > 2:
                self._print_report(results)
                last_report_time = current_time

            frame_count += 1

            # é€€å‡ºé”®
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    def analyze_frame(self, image_rgb, image_bgr=None):
        """
        åˆ†æè§†é¢‘å¸§ï¼ˆåŒæ—¶è¿›è¡Œé¢éƒ¨å’Œè‚¢ä½“åˆ†æï¼‰

        å‚æ•°:
            image_rgb: RGB æ ¼å¼çš„å›¾åƒ
            image_bgr: BGR æ ¼å¼çš„å›¾åƒï¼ˆç”¨äºå¯è§†åŒ–ï¼Œå¯é€‰ï¼‰

        è¿”å›:
            åŒ…å«æ‰€æœ‰åˆ†æç»“æœçš„å­—å…¸
        """
        results = {
            "face": {"is_valid": False},
            "hand": {"is_valid": False},
            "shoulder": {"is_valid": False, "is_calibrated": False},
            "emotion": {"overall_score": 50.0, "emotion_state": "ä¸­æ€§", "emoji": "ğŸŸ¡", "feedback": "åˆ†æä¸­", "color": (255, 255, 0)}
        }

        try:
            # é¢éƒ¨åˆ†æ
            if self.face_analyzer:
                features, mesh_results, emotion = self.face_analyzer.process_frame(image_rgb)
                face_result = {
                    "is_valid": features is not None,
                    "features": features if features else {},
                    "emotion": emotion,
                    "mesh_results": mesh_results
                }
                results["face"] = face_result
                self.current_results["face"] = face_result

            # æ‰‹åŠ¿å’Œè‚©éƒ¨åˆ†æ
            hand_result, shoulder_result = self._analyze_gesture(image_rgb, image_bgr)
            results["hand"] = hand_result
            results["shoulder"] = shoulder_result
            self.current_results["hand"] = hand_result
            self.current_results["shoulder"] = shoulder_result

            # èåˆæƒ…ç»ªè¯„ä¼°
            emotion_result = self._fuse_emotion(face_result, hand_result, shoulder_result)
            results["emotion"] = emotion_result
            self.current_results["emotion"] = emotion_result

        except Exception as e:
            print(f"âš ï¸ å¸§åˆ†æå¤±è´¥: {e}")

        return results

    def _analyze_gesture(self, image_rgb, image_bgr=None):
        """åˆ†ææ‰‹åŠ¿å’Œè‚©éƒ¨"""
        hand_result = {"resilience_score": 50.0, "is_valid": False}
        shoulder_result = {"shoulder_score": 50.0, "is_valid": False, "is_calibrated": False}

        try:
            # æ‰‹éƒ¨æ£€æµ‹
            hands_results = self.mp_hands.process(image_rgb)
            if hands_results.multi_hand_landmarks and hands_results.multi_handedness:
                for idx, (landmarks, handedness) in enumerate(
                    zip(hands_results.multi_hand_landmarks, hands_results.multi_handedness)
                ):
                    if idx >= 2:
                        break
                    label = handedness.classification[0].label
                    analyzer = self.hand_analyzer_left if label == "Left" else self.hand_analyzer_right
                    analyzer.update(landmarks.landmark)
                    if label == "Left":
                        hand_result = analyzer.get_results()
                    else:
                        hand_result = analyzer.get_results()

                    # ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹ï¼ˆå¦‚æœæä¾›äº†BGRå›¾åƒï¼‰
                    if image_bgr is not None:
                        image_bgr = self.gesture_visualizer.draw_hand_landmarks(image_bgr, landmarks)

            # è‚©éƒ¨æ£€æµ‹
            pose_results = self.mp_pose.process(image_rgb)
            if pose_results.pose_landmarks:
                self.shoulder_analyzer.update(pose_results.pose_landmarks.landmark)
                shoulder_result = self.shoulder_analyzer.get_results()
                # ç»˜åˆ¶è‚©éƒ¨å…³é”®ç‚¹
                if image_bgr is not None:
                    image_bgr = self.gesture_visualizer.draw_pose_landmarks(image_bgr, pose_results.pose_landmarks)

        except Exception as e:
            print(f"âš ï¸ æ‰‹åŠ¿åˆ†æå¤±è´¥: {e}")

        return hand_result, shoulder_result

    def _handle_voice_interaction(self):
        """å¤„ç†è¯­éŸ³äº¤äº’"""
        try:
            # è·å–ä¸‹ä¸€ä¸ªé—®é¢˜
            question = self.interview_assessment.get_next_question()
            if question is None:
                return

            print(f"\né—®é¢˜ {self.current_question_index + 1}: {question}")
            if self.tts_engine and self.tts_engine.is_available():
                self.tts_engine.speak(question)
                self.tts_engine.speak("è¯·å¼€å§‹å›ç­”ã€‚")

            # è·å–å›ç­”
            answer = ""
            if self.speech_recognizer:
                answer = self.speech_recognizer.listen_for_speech()

            if not answer:
                answer = "[æ— æœ‰æ•ˆå›ç­”]"

            # è®°å½•å›ç­”
            self.interview_assessment.add_answer(answer)
            self.current_results["voice"] = {"text": answer, "is_valid": True}
            self.current_question_index += 1

        except Exception as e:
            print(f"âš ï¸ è¯­éŸ³äº¤äº’å¤±è´¥: {e}")

    def _fuse_emotion(self, face_result, hand_result, shoulder_result):
        """èåˆå¤šæ¨¡æ€æƒ…ç»ªè¯„ä¼°"""
        # è·å–å„æ¨¡å—è¯„åˆ†
        face_score = 50.0
        if face_result.get("is_valid", False):
            features = face_result.get("features", {})
            if features:
                # ä½¿ç”¨ä¸“æ³¨åº¦ä½œä¸ºé¢éƒ¨è¯„åˆ†
                face_score = features.get("focus_score", 50.0) * 100

        hand_score = hand_result.get("resilience_score", 50.0) if hand_result.get("is_valid", False) else 50.0
        shoulder_score = shoulder_result.get("shoulder_score", 50.0) if shoulder_result.get("is_valid", False) else 50.0

        # åŠ æƒèåˆï¼ˆä½ å¯ä»¥è°ƒæ•´æƒé‡ï¼‰
        overall_score = (
            face_score * 0.3 +
            hand_score * 0.4 +
            shoulder_score * 0.3
        )

        # æ˜ å°„åˆ°æƒ…ç»ªçŠ¶æ€
        if overall_score >= 80:
            emotion_state = "éå¸¸æ”¾æ¾"
            emoji = "ğŸŸ¢"
            color = (0, 255, 0)
        elif overall_score >= 65:
            emotion_state = "æ”¾æ¾"
            emoji = "ğŸŸ¢"
            color = (0, 255, 0)
        elif overall_score >= 50:
            emotion_state = "ä¸­æ€§"
            emoji = "ğŸŸ¡"
            color = (255, 255, 0)
        elif overall_score >= 35:
            emotion_state = "è½»å¾®ç´§å¼ "
            emoji = "ğŸŸ "
            color = (255, 165, 0)
        else:
            emotion_state = "ç´§å¼ "
            emoji = "ğŸ”´"
            color = (0, 0, 255)

        feedback = f"ç»¼åˆæƒ…ç»ªè¯„åˆ†: {overall_score:.1f}/100 â†’ {emoji} {emotion_state}"

        return {
            "overall_score": float(np.clip(overall_score, 0, 100)),
            "emotion_state": emotion_state,
            "emoji": emoji,
            "feedback": feedback,
            "color": color
        }

    def _visualize_all(self, image, results):
        """å¯è§†åŒ–æ‰€æœ‰åˆ†æç»“æœ"""
        try:
            # é¢éƒ¨å¯è§†åŒ–
            if results["face"].get("is_valid", False):
                mesh_results = results["face"].get("mesh_results")
                if mesh_results and mesh_results.multi_face_landmarks:
                    try:
                        import mediapipe as mp
                        mp_drawing = mp.solutions.drawing_utils
                        mp_face_mesh = mp.solutions.face_mesh

                        # ç»˜åˆ¶é¢éƒ¨ç½‘æ ¼
                        mp_drawing.draw_landmarks(
                            image=image,
                            landmark_list=mesh_results.multi_face_landmarks[0],
                            connections=mp_face_mesh.FACEMESH_TESSELATION,
                            landmark_drawing_spec=None,
                            connection_drawing_spec=mp_drawing.DrawingSpec(
                                color=(80, 110, 10), thickness=1, circle_radius=1
                            )
                        )
                    except Exception as e:
                        print(f"âš ï¸ é¢éƒ¨ç½‘æ ¼ç»˜åˆ¶å¤±è´¥: {e}")

                # æ˜¾ç¤ºé¢éƒ¨è¡¨æƒ…ä¿¡æ¯
                emotion = results["face"].get("emotion", "æœªçŸ¥")
                features = results["face"].get("features", {})
                if features:
                    # ä½¿ç”¨OpenCVç»˜åˆ¶æ‰€æœ‰AUç‰¹å¾ï¼ˆä¸å•ç‹¬è¿è¡Œçš„é¢éƒ¨æ¨¡å—ä¸€è‡´ï¼‰
                    lines = [
                        f"Emotion: {emotion}",
                        f"Focus: {features.get('focus_score', 0.5):.2f}",
                        f"Blink: {features.get('blink_rate_per_min', 0.0):.1f}/min",
                        f"AU4_Frown: {features.get('au4_frown', 0.0):.3f}",
                        f"AU12_Raise: {features.get('au12_eyebrow_raise', 0.0):.3f}",
                        f"AU12_Smile: {features.get('au12_smile', 0.0):.3f}",
                        f"AU9_Wrinkle: {features.get('au9_nose_wrinkle', 0.0):.3f}",
                        f"AU15_Down: {features.get('au15_mouth_down', 0.0):.3f}",
                        f"AU25_Open: {features.get('au25_mouth_open', 0.0):.3f}",
                        f"EyeClosed: {features.get('eye_closed_sec', 0.0):.1f}s"
                    ]
                    for i, line in enumerate(lines):
                        cv2.putText(image, line, (10, 30 + i * 25),
                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)

            # æ‰‹åŠ¿/è‚©éƒ¨å¯è§†åŒ–
            if self.gesture_visualizer:
                # æ˜¾ç¤ºæ‰‹åŠ¿åˆ†æ•°ï¼ˆå³ä¾§ï¼‰
                if results["hand"].get("is_valid", False):
                    image = self.gesture_visualizer.put_chinese_text(
                        image, f"å·¦æ‰‹: {results['hand']['resilience_score']:.1f}", (400, 30), color=(0, 255, 0)
                    )
                    image = self.gesture_visualizer.put_chinese_text(
                        image, f"å³æ‰‹: {results['hand']['resilience_score']:.1f}", (400, 60), color=(0, 255, 0)
                    )

                # æ˜¾ç¤ºè‚©éƒ¨åˆ†æ•°ï¼ˆå³ä¾§ï¼‰
                if results["shoulder"].get("is_valid", False):
                    shoulder_text = f"è‚©éƒ¨: {results['shoulder']['shoulder_score']:.1f}"
                    if not results["shoulder"].get("is_calibrated", False):
                        shoulder_text += " (æ ¡å‡†ä¸­...)"
                    image = self.gesture_visualizer.put_chinese_text(
                        image, shoulder_text, (400, 90), color=(0, 255, 255)
                    )

                # ç»¼åˆæƒ…ç»ªï¼ˆåº•éƒ¨ï¼‰
                image = self.gesture_visualizer.draw_emotion_result(image, results["emotion"], position=(10, 430))

        except Exception as e:
            print(f"âš ï¸ å¯è§†åŒ–å¤±è´¥: {e}")

        return image

    def get_answer(self):
        """è·å–å½“å‰å›ç­”"""
        return self.current_results["voice"].get("text", "")

    def get_comprehensive_evaluation(self):
        """è·å–ç»¼åˆè¯„ä¼°"""
        if self.interview_assessment:
            return self.interview_assessment.get_comprehensive_evaluation()
        return {"text": "è¯„ä¼°ä¸å¯ç”¨", "is_valid": False}

    def _print_report(self, results):
        """æ‰“å°ç»ˆç«¯æŠ¥å‘Š"""
        print("\n" + "="*60)
        print(f"ğŸ“Š å¤šæ¨¡æ€åˆ†ææŠ¥å‘Š ({time.strftime('%Y-%m-%d %H:%M:%S')})")
        print("="*60)
        print(f"é¢éƒ¨è¡¨æƒ…: {'æœ‰æ•ˆ' if results['face'].get('is_valid', False) else 'æ— æ•ˆ'}")
        print(f"æ‰‹åŠ¿åˆ†æ: {'æœ‰æ•ˆ' if results['hand'].get('is_valid', False) else 'æ— æ•ˆ'}")
        print(f"è‚©éƒ¨åˆ†æ: {'æœ‰æ•ˆ' if results['shoulder'].get('is_valid', False) else 'æ— æ•ˆ'}")
        print(f"ç»¼åˆæƒ…ç»ª: {results['emotion']['overall_score']:.1f}/100 â†’ {results['emotion']['emoji']} {results['emotion']['emotion_state']}")
        print("="*60)

    def stop(self):
        """åœæ­¢å¹¶æ¸…ç†èµ„æº"""
        self.running = False
        if self.cap:
            self.cap.release()
        if self.mp_hands:
            self.mp_hands.close()
        if self.mp_pose:
            self.mp_pose.close()
        if self.tts_engine:
            self.tts_engine.stop()
        cv2.destroyAllWindows()
        print("\nâœ… å¤šæ¨¡æ€ç³»ç»Ÿå·²å®‰å…¨é€€å‡ºã€‚")


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå¤šæ¨¡æ€é›†æˆç³»ç»Ÿ"""
    integrator = None
    try:
        integrator = JingXinIntegrator()
        integrator.start_interview_session()
    except Exception as e:
        print(f"âŒ è¿è¡Œæ—¶é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if integrator:
            integrator.stop()


if __name__ == "__main__":
    main()