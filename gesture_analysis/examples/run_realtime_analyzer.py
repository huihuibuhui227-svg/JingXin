"""
å®æ—¶æ‰‹åŠ¿ä¸è‚©éƒ¨æƒ…ç»ªè¯„ä¼°ç¤ºä¾‹è„šæœ¬

æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ gesture_analysis æ¨¡å—è¿›è¡Œå®æ—¶è§†é¢‘æµåˆ†æã€‚
å®Œå…¨å¤ç”¨ analyzers / inference / utils æ¨¡å—ï¼Œé¿å…é‡å¤ä»£ç ã€‚
"""

import cv2
import time
import signal
import sys
from datetime import datetime

# æ ‡å‡†å¯¼å…¥ï¼ˆå‡è®¾ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼špython -m gesture_analysis.examples.run_realtime_analyzerï¼‰
try:
    from gesture_analysis.analyzers import HandAnalyzer, ShoulderAnalyzer
    from gesture_analysis.inference import EmotionInferencer
    from gesture_analysis.utils import Visualizer, GestureLogger
    from gesture_analysis.config import MEDIAPIPE_CONFIG
except ImportError as e:
    print("âŒ å¯¼å…¥å¤±è´¥ï¼è¯·ç¡®ä¿ä»é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼š")
    print("   python -m gesture_analysis.examples.run_realtime_analyzer")
    print(f"   é”™è¯¯è¯¦æƒ…: {e}")
    sys.exit(1)

import mediapipe as mp


class RealtimeAnalyzer:
    """å®æ—¶åˆ†æå™¨å°è£…ç±»ï¼Œä¾¿äºèµ„æºç®¡ç†å’Œå¼‚å¸¸å¤„ç†"""

    def __init__(self):
        self.hands = None
        self.pose = None
        self.cap = None
        self.running = False

        # åˆå§‹åŒ–åˆ†æå™¨
        self.left_hand_analyzer = HandAnalyzer(hand_id=0)
        self.right_hand_analyzer = HandAnalyzer(hand_id=1)
        self.shoulder_analyzer = ShoulderAnalyzer()
        self.emotion_inferencer = EmotionInferencer()

        # åˆå§‹åŒ–å·¥å…·
        self.visualizer = Visualizer()
        self.logger = GestureLogger()

        # åˆå§‹åŒ– MediaPipe
        self._init_mediapipe()

    def _init_mediapipe(self):
        """åˆå§‹åŒ– MediaPipe æ¨¡å‹"""
        mp_hands = mp.solutions.hands
        mp_pose = mp.solutions.pose

        self.hands = mp_hands.Hands(**MEDIAPIPE_CONFIG['hands'])
        self.pose = mp_pose.Pose(**MEDIAPIPE_CONFIG['pose'])

    def start(self):
        """å¯åŠ¨å®æ—¶åˆ†æ"""
        print("=" * 60)
        print("å®æ—¶æ‰‹åŠ¿ä¸è‚©éƒ¨æƒ…ç»ªè¯„ä¼°ç³»ç»Ÿ")
        print("=" * 60)
        print("â„¹ï¸  å¯åŠ¨åè¯·ä¿æŒè‡ªç„¶åå§¿1~2ç§’ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨æ ¡å‡†è‚©éƒ¨åŸºå‡†")
        print("â„¹ï¸  æ ¡å‡†å®Œæˆåï¼Œè€¸è‚©å°†è¢«æ­£ç¡®æ£€æµ‹")
        print("ğŸ›‘ æŒ‰ 'q' é”®æˆ– Ctrl+C é€€å‡ºç¨‹åº\n")

        # æ‰“å¼€æ‘„åƒå¤´
        self.cap = cv2.VideoCapture(0)
        if not self.cap.isOpened():
            raise RuntimeError("æ— æ³•æ‰“å¼€æ‘„åƒå¤´ï¼")

        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

        # æ³¨å†Œä¿¡å·å¤„ç†å™¨ï¼ˆæ”¯æŒ Ctrl+Cï¼‰
        signal.signal(signal.SIGINT, self._signal_handler)

        self.running = True
        self._main_loop()

    def _signal_handler(self, signum, frame):
        """å¤„ç†ä¸­æ–­ä¿¡å·"""
        print("\nâš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨é€€å‡º...")
        self.running = False

    def _main_loop(self):
        """ä¸»åˆ†æå¾ªç¯"""
        frame_count = 0
        last_report_time = time.time()

        while self.running and self.cap.isOpened():
            success, image = self.cap.read()
            if not success:
                continue

            # é•œåƒç¿»è½¬ + è½¬ RGB
            image = cv2.flip(image, 1)
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # === 1. æ‰‹éƒ¨åˆ†æ ===
            hand_results_raw = self.hands.process(image_rgb)
            left_result = {"resilience_score": 50.0, "is_valid": False}
            right_result = {"resilience_score": 50.0, "is_valid": False}

            if hand_results_raw.multi_hand_landmarks and hand_results_raw.multi_handedness:
                for idx, (landmarks, handedness) in enumerate(
                    zip(hand_results_raw.multi_hand_landmarks, hand_results_raw.multi_handedness)
                ):
                    if idx >= 2:  # æœ€å¤šå¤„ç†ä¸¤åªæ‰‹
                        break
                    label = handedness.classification[0].label  # 'Left' or 'Right'
                    analyzer = self.left_hand_analyzer if label == "Left" else self.right_hand_analyzer
                    analyzer.update(landmarks.landmark)
                    if label == "Left":
                        left_result = analyzer.get_results()
                    else:
                        right_result = analyzer.get_results()

                    # ç»˜åˆ¶å…³é”®ç‚¹
                    image = self.visualizer.draw_hand_landmarks(image, landmarks)

            # === 2. è‚©éƒ¨åˆ†æ ===
            shoulder_result = {"shoulder_score": 50.0, "is_valid": False, "is_calibrated": False}
            pose_results_raw = self.pose.process(image_rgb)
            if pose_results_raw.pose_landmarks:
                self.shoulder_analyzer.update(pose_results_raw.pose_landmarks.landmark)
                shoulder_result = self.shoulder_analyzer.get_results()
                image = self.visualizer.draw_pose_landmarks(image, pose_results_raw.pose_landmarks)

            # === 3. æƒ…ç»ªæ¨æ–­ ===
            emotion_result = self.emotion_inferencer.infer_emotion(left_result, shoulder_result)

            # === 4. å¯è§†åŒ– ===
            image = self.visualizer.draw_emotion_result(image, emotion_result, position=(10, 200))

            # æ˜¾ç¤ºå·¦å³æ‰‹è¯„åˆ†ï¼ˆä»…å½“æœ‰æ•ˆæ—¶ï¼‰
            if left_result["is_valid"]:
                image = self.visualizer.put_chinese_text(
                    image, f"å·¦æ‰‹: {left_result['resilience_score']:.1f}", (10, 30), color=(0, 255, 0)
                )
            if right_result["is_valid"]:
                image = self.visualizer.put_chinese_text(
                    image, f"å³æ‰‹: {right_result['resilience_score']:.1f}", (400, 30), color=(0, 255, 0)
                )

            # æ˜¾ç¤ºè‚©éƒ¨çŠ¶æ€
            shoulder_text = f"è‚©éƒ¨: {shoulder_result['shoulder_score']:.1f}"
            if not shoulder_result["is_calibrated"]:
                shoulder_text += " (æ ¡å‡†ä¸­...)"
            image = self.visualizer.put_chinese_text(
                image, shoulder_text, (10, 60), color=(0, 255, 255)
            )

            cv2.imshow('Gesture & Shoulder Emotion Analyzer', image)

            # === 5. æ—¥å¿—è®°å½• ===
            self.logger.log(left_result, right_result, shoulder_result, emotion_result)

            # === 6. ç»ˆç«¯æŠ¥å‘Šï¼ˆæ¯2ç§’ï¼‰===
            current_time = time.time()
            if current_time - last_report_time > 2:
                self._print_report(left_result, right_result, shoulder_result, emotion_result)
                last_report_time = current_time

            frame_count += 1

            # é€€å‡ºé”®
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    def _print_report(self, left, right, shoulder, emotion):
        """æ‰“å°ç»ˆç«¯æŠ¥å‘Š"""
        print("\n" + "="*60)
        print(f"ğŸ“Š æƒ…ç»ªè¯„ä¼°æŠ¥å‘Šï¼ˆ{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}ï¼‰")
        print("="*60)
        print(f"å·¦æ‰‹: {left['resilience_score']:.1f} | å³æ‰‹: {right['resilience_score']:.1f}")
        print(f"è‚©éƒ¨: {shoulder['shoulder_score']:.1f} {'(æ ¡å‡†ä¸­)' if not shoulder['is_calibrated'] else ''}")
        print(f"ç»¼åˆæƒ…ç»ª: {emotion['overall_score']:.1f}/100 â†’ {emotion['emoji']} {emotion['emotion_state']}")
        print(f"å»ºè®®: {emotion['feedback']}")
        print(f"æ•°æ®æ¥æº: {emotion['used_features']} | æœ‰æ•ˆ: {emotion['is_valid']}")
        print("="*60)

    def stop(self):
        """åœæ­¢å¹¶æ¸…ç†èµ„æº"""
        self.running = False
        if self.cap:
            self.cap.release()
        if self.hands:
            self.hands.close()
        if self.pose:
            self.pose.close()
        cv2.destroyAllWindows()
        print("\nâœ… ç¨‹åºå·²å®‰å…¨é€€å‡ºã€‚")


def main():
    analyzer = None
    try:
        analyzer = RealtimeAnalyzer()
        analyzer.start()
    except Exception as e:
        print(f"âŒ è¿è¡Œæ—¶é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if analyzer:
            analyzer.stop()


if __name__ == "__main__":
    main()